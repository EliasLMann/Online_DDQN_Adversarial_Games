{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Opponent Modeling with LOLA for Adversarial Environments\n",
    "\n",
    "Elias Mann, Max Hasenauer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "from pettingzoo.classic import tictactoe_v3, rps_v2, connect_four_v3\n",
    "from collections import namedtuple\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environments\n",
    "\n",
    "We are using simple adversarial environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = {\n",
    "    'rps': rps_v2.env(render_mode='rgb_array'),\n",
    "    'tictactoe': tictactoe_v3.env(render_mode='rgb_array'),\n",
    "    'connect_four': connect_four_v3.env(render_mode='rgb_array')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render(env, policy=None, seed=42):\n",
    "\n",
    "    env.reset(seed=seed)\n",
    "\n",
    "\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        obs, reward, done, info, _ = env.last()\n",
    "        if done:\n",
    "            action = None\n",
    "        else:\n",
    "            mask = obs['action_mask']\n",
    "            if policy is None:\n",
    "                action = env.action_spaces[agent].sample(mask)\n",
    "            else:\n",
    "                action = policy(mask, env.action_spaces[agent])\n",
    "\n",
    "        env.step(action)\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "render(envs['tictactoe'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "### Starting with the DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExponentialSchedule:\n",
    "    def __init__(self, value_from, value_to, num_steps):\n",
    "        \"\"\"Exponential schedule from `value_from` to `value_to` in `num_steps` steps.\n",
    "\n",
    "        $value(t) = a \\exp (b t)$\n",
    "\n",
    "        :param value_from: initial value\n",
    "        :param value_to: final value\n",
    "        :param num_steps: number of steps for the exponential schedule\n",
    "        \"\"\"\n",
    "        self.value_from = value_from\n",
    "        self.value_to = value_to\n",
    "        self.num_steps = num_steps\n",
    "\n",
    "        # YOUR CODE HERE:  determine the `a` and `b` parameters such that the schedule is correct\n",
    "        self.a = value_to / value_from\n",
    "        self.b = math.log(self.a) / (num_steps - 1)\n",
    "\n",
    "    def value(self, step) -> float:\n",
    "        \"\"\"Return exponentially interpolated value between `value_from` and `value_to`interpolated value between.\n",
    "\n",
    "        returns {\n",
    "            `value_from`, if step == 0 or less\n",
    "            `value_to`, if step == num_steps - 1 or more\n",
    "            the exponential interpolation between `value_from` and `value_to`, if 0 <= steps < num_steps\n",
    "        }\n",
    "\n",
    "        :param step:  The step at which to compute the interpolation.\n",
    "        :rtype: float.  The interpolated value.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE:  implement the schedule rule as described in the docstring,\n",
    "        # using attributes `self.a` and `self.b`.\n",
    "        if step <= 0:\n",
    "            value = self.value_from\n",
    "        elif step >= self.num_steps - 1:\n",
    "            value = self.value_to\n",
    "        else:\n",
    "            value = self.value_from * math.exp(self.b * step)\n",
    "\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch namedtuple, i.e. a class which contains the given attributes\n",
    "Batch = namedtuple(\n",
    "    'Batch', ('states', 'actions', 'rewards', 'next_states', 'dones')\n",
    ")\n",
    "\n",
    "\n",
    "class ReplayMemory:\n",
    "    def __init__(self, max_size, state_size):\n",
    "        \"\"\"Replay memory implemented as a circular buffer.\n",
    "\n",
    "        Experiences will be removed in a FIFO manner after reaching maximum\n",
    "        buffer size.\n",
    "\n",
    "        Args:\n",
    "            - max_size: Maximum size of the buffer.\n",
    "            - state_size: Size of the state-space features for the environment.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.state_size = state_size\n",
    "\n",
    "        # preallocating all the required memory, for speed concerns\n",
    "        self.states = torch.empty((max_size, state_size))\n",
    "        self.actions = torch.empty((max_size, 1), dtype=torch.long)\n",
    "        self.rewards = torch.empty((max_size, 1))\n",
    "        self.next_states = torch.empty((max_size, state_size))\n",
    "        self.dones = torch.empty((max_size, 1), dtype=torch.bool)\n",
    "\n",
    "        # pointer to the current location in the circular buffer\n",
    "        self.idx = 0\n",
    "        # indicates number of transitions currently stored in the buffer\n",
    "        self.size = 0\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a transition to the buffer.\n",
    "\n",
    "        :param state:  1-D np.ndarray of state-features.\n",
    "        :param action:  integer action.\n",
    "        :param reward:  float reward.\n",
    "        :param next_state:  1-D np.ndarray of state-features.\n",
    "        :param done:  boolean value indicating the end of an episode.\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE:  store the input values into the appropriate\n",
    "        # attributes, using the current buffer position `self.idx`\n",
    "        self.states[self.idx] = torch.tensor(state.flatten(), dtype=torch.float)\n",
    "        self.actions[self.idx] = torch.tensor(action, dtype=torch.long)\n",
    "        self.rewards[self.idx] = torch.tensor(reward, dtype=torch.float)\n",
    "        self.next_states[self.idx] = torch.tensor(next_state.flatten(), dtype=torch.float)\n",
    "        self.dones[self.idx] = torch.tensor(done, dtype=torch.bool)\n",
    "        \n",
    "        # DO NOT EDIT\n",
    "        # circulate the pointer to the next position\n",
    "        self.idx = (self.idx + 1) % self.max_size\n",
    "        # update the current buffer size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample(self, batch_size) -> Batch:\n",
    "        \"\"\"Sample a batch of experiences.\n",
    "\n",
    "        If the buffer contains less that `batch_size` transitions, sample all\n",
    "        of them.\n",
    "\n",
    "        :param batch_size:  Number of transitions to sample.\n",
    "        :rtype: Batch\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE:  randomly sample an appropriate number of\n",
    "        # transitions *without replacement*.  If the buffer contains less than\n",
    "        # `batch_size` transitions, return all of them.  The return type must\n",
    "        # be a `Batch`.\n",
    "\n",
    "        sample_indices = torch.randint(0, self.size, size=(batch_size,))\n",
    "        batch = Batch(\n",
    "            states=self.states[sample_indices],\n",
    "            actions=self.actions[sample_indices],\n",
    "            rewards=self.rewards[sample_indices],\n",
    "            next_states=self.next_states[sample_indices],\n",
    "            dones=self.dones[sample_indices]\n",
    "        )\n",
    "\n",
    "        return batch\n",
    "\n",
    "    def populate(self, env, num_steps):\n",
    "        \"\"\"Populate this replay memory with `num_steps` from the random policy.\n",
    "\n",
    "        :param env:  Openai Gym environment\n",
    "        :param num_steps:  Number of steps to populate the\n",
    "        \"\"\"\n",
    "\n",
    "        # YOUR CODE HERE:  run a random policy for `num_steps` time-steps and\n",
    "        # populate the replay memory with the resulting transitions.\n",
    "        # Hint:  don't repeat code!  Use the self.add() method!\n",
    "        #randomly choose 1 or -1 to decide which agent to play\n",
    "        curr_player = np.random.choice([env.possible_agents[0], env.possible_agents[1]])\n",
    "            \n",
    "        env.reset()\n",
    "        step_count = 0\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, done, _, _ = env.last()\n",
    "            if done:\n",
    "                env.reset()\n",
    "                curr_player = np.random.choice([env.possible_agents[0], env.possible_agents[1]])\n",
    "            else:\n",
    "                mask = obs['action_mask']\n",
    "                action = env.action_spaces[agent].sample(mask)\n",
    "                env.step(action)\n",
    "                next_state, reward, _, _,_ = env.last()\n",
    "                reward = reward if agent == curr_player else -reward\n",
    "                self.add(obs['observation'], action, reward, next_state['observation'], done)            \n",
    "                \n",
    "            step_count += 1\n",
    "            if step_count >= num_steps:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, *, num_layers=3, hidden_dim=128):\n",
    "\n",
    "        super().__init__()\n",
    "        #check if observation is a tuple\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(self.state_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        for _ in range(num_layers - 2):  # Subtract 2 for the input and output layers\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        layers.append(nn.Linear(hidden_dim, action_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, states) -> torch.Tensor:\n",
    "\n",
    "        #assert that this is not an empty tensor\n",
    "        assert len(states.shape) > 0, f\"Input states tensor has invalid shape {states}\"\n",
    "        assert states.shape[-1] == self.state_dim, f\"Input states shape {states.shape} does not match expected state_dim {self.state_dim}\"\n",
    "        action_values = self.model(states)\n",
    "        return action_values\n",
    "    \n",
    "    # utility methods for cloning and storing models.  DO NOT EDIT\n",
    "    @classmethod\n",
    "    def custom_load(cls, data):\n",
    "        model = cls(*data['args'], **data['kwargs'])\n",
    "        model.load_state_dict(data['state_dict'])\n",
    "        return model\n",
    "\n",
    "    def custom_dump(self):\n",
    "        return {\n",
    "            'args': (self.state_dim, self.action_dim),\n",
    "            'kwargs': {\n",
    "                'num_layers': self.num_layers,\n",
    "                'hidden_dim': self.hidden_dim,\n",
    "            },\n",
    "            'state_dict': self.state_dict(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_batch(optimizer, batch, dqn_model, dqn_target, gamma) -> float:\n",
    "    \"\"\"Perform a single batch-update step on the given DQN model.\n",
    "\n",
    "    :param optimizer: nn.optim.Optimizer instance.\n",
    "    :param batch:  Batch of experiences (class defined earlier).\n",
    "    :param dqn_model:  The DQN model to be trained.\n",
    "    :param dqn_target:  The target DQN model, ~NOT~ to be trained.\n",
    "    :param gamma:  The discount factor.\n",
    "    :rtype: float  The scalar loss associated with this batch.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE:  compute the values and target_values tensors using the\n",
    "    # given models and the batch of data.\n",
    "    values = dqn_model(batch.states).gather(1, batch.actions)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Select the maximum Q-value for each next state\n",
    "        max_next_values = dqn_target(batch.next_states).max(dim=1, keepdim=True)[0]\n",
    "        # Compute the TD target: reward + gamma * max_next_value * (1 - done)\n",
    "        target_values = batch.rewards + gamma * max_next_values *  ~batch.dones\n",
    "\n",
    "\n",
    "    # DO NOT EDIT FURTHER\n",
    "\n",
    "    assert (\n",
    "        values.shape == target_values.shape\n",
    "    ), 'Shapes of values tensor and target_values tensor do not match.'\n",
    "\n",
    "    # testing that the value tensor requires a gradient,\n",
    "    # and the target_values tensor does not\n",
    "    assert values.requires_grad, 'values tensor should not require gradients'\n",
    "    assert (\n",
    "        not target_values.requires_grad\n",
    "    ), 'target_values tensor should require gradients'\n",
    "\n",
    "    # computing the scalar MSE loss between computed values and the TD-target\n",
    "    loss = F.mse_loss(values, target_values)\n",
    "\n",
    "    optimizer.zero_grad()  # reset all previous gradients\n",
    "    loss.backward()  # compute new gradients\n",
    "    optimizer.step()  # perform one gradient descent step\n",
    "\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_dqn_adversarial(\n",
    "    env,\n",
    "    num_steps,\n",
    "    *,\n",
    "    num_saves=5,\n",
    "    replay_size,\n",
    "    replay_prepopulate_steps=0,\n",
    "    batch_size,\n",
    "    exploration,\n",
    "    gamma,\n",
    "):\n",
    "    \"\"\"\n",
    "    DQN algorithm.\n",
    "\n",
    "    Compared to previous training procedures, we will train for a given number\n",
    "    of time-steps rather than a given number of episodes.  The number of\n",
    "    time-steps will be in the range of millions, which still results in many\n",
    "    episodes being executed.\n",
    "\n",
    "    Args:\n",
    "        - env: The openai Gym environment\n",
    "        - num_steps: Total number of steps to be used for training\n",
    "        - num_saves: How many models to save to analyze the training progress.\n",
    "        - replay_size: Maximum size of the ReplayMemory\n",
    "        - replay_prepopulate_steps: Number of steps with which to prepopulate\n",
    "                                    the memory\n",
    "        - batch_size: Number of experiences in a batch\n",
    "        - exploration: a ExponentialSchedule\n",
    "        - gamma: The discount factor\n",
    "\n",
    "    Returns: (saved_models, returns)\n",
    "        - saved_models: Dictionary whose values are trained DQN models\n",
    "        - returns: Numpy array containing the return of each training episode\n",
    "        - lengths: Numpy array containing the length of each training episode\n",
    "        - losses: Numpy array containing the loss of each training batch\n",
    "    \"\"\"\n",
    "    # initialize the DQN and DQN-target models\n",
    "    dqn_models = {}\n",
    "    dqn_targets = {}\n",
    "    # initialize the optimizer\n",
    "    optimizers = {}\n",
    "    # initialize the replay memory and prepopulate it\n",
    "    memories  = {}\n",
    "    # initiate lists to store returns, lengths and losses\n",
    "    returns = {}\n",
    "    lengths = {}\n",
    "    losses = {}\n",
    "\n",
    "    saved_models = {}\n",
    "\n",
    "\n",
    "\n",
    "    for agent in ['agent_0', 'agent_1']:\n",
    "        # check that the observation space is a Box\n",
    "        observation_space = np.prod(env.observation_spaces[env.possible_agents[0]]['observation'].shape)\n",
    "        action_space = env.action_space(env.possible_agents[0])\n",
    "        dqn_models[agent] = DQN(observation_space, action_space.n)\n",
    "        dqn_targets[agent] = DQN.custom_load(dqn_models[agent].custom_dump())\n",
    "        optimizers[agent] = optim.Adam(dqn_models[agent].parameters())\n",
    "        memories[agent] = ReplayMemory(replay_size, observation_space)\n",
    "        memories[agent].populate(env, replay_prepopulate_steps)\n",
    "        returns[agent] = []\n",
    "        lengths[agent] = []\n",
    "        losses[agent] = []\n",
    "        saved_models[agent] = {}\n",
    "\n",
    "    # initiate structures to store the models at different stages of training\n",
    "    t_saves = np.linspace(0, num_steps, num_saves - 1, endpoint=False)\n",
    "    i_episode = 0  # use this to indicate the index of the current episode\n",
    "    t_episode = 0  # use this to indicate the time-step inside current episode\n",
    "\n",
    "    # iterate for a total of `num_steps` steps\n",
    "    pbar = tqdm.trange(num_steps, ncols=100)\n",
    "    for t_total in pbar:\n",
    "        # use t_total to indicate the time-step from the beginning of training\n",
    "\n",
    "        # save models\n",
    "        if t_total in t_saves:\n",
    "            model_name_0 = f'{100 * t_total / num_steps:04.1f}'.replace('.', '_')\n",
    "            saved_models['agent_0'][model_name_0] = copy.deepcopy(dqn_models['agent_0'])\n",
    "\n",
    "            model_name_1 = f'{100 * t_total / num_steps:04.1f}'.replace('.', '_')\n",
    "            saved_models['agent_1'][model_name_1] = copy.deepcopy(dqn_models['agent_0'])\n",
    "\n",
    "        env.reset()\n",
    "        #randomly choose which agent is which player\n",
    "        players = ['agent_0', 'agent_1']\n",
    "        np.random.shuffle(players) \n",
    "        player_0, player_1 = players\n",
    "        # Sample an action from the DQN using epsilon-greedy\n",
    "        for agent in env.agent_iter():\n",
    "\n",
    "            #assign the current agent to the player\n",
    "            curr_agent = player_0 if agent == 'player_0' else player_1 \n",
    "\n",
    "            # Get the current state\n",
    "            obs, reward, done, _, _ = env.last()\n",
    "\n",
    "            if done:\n",
    "                #add the last transition to the replay memory\n",
    "                memories[curr_agent].add(obs['observation'], action, reward, next_state['observation'], done)\n",
    "\n",
    "                for final_agent in ['agent_0', 'agent_1']:\n",
    "                    # Compute return G\n",
    "                    G = 0\n",
    "                    for i in range(t_episode, -1, -1):\n",
    "                        G = gamma * G + memories[final_agent].rewards[i]\n",
    "                        \n",
    "                    # Store stuff\n",
    "                    returns[final_agent].append(G)\n",
    "                    lengths[final_agent].append(t_episode + 1)\n",
    "        \n",
    "                # Reset variables, indices, lists, etc.\n",
    "                i_episode += 1\n",
    "                t_episode = 0\n",
    "\n",
    "                # Update the progress bar\n",
    "                pbar.set_description(\n",
    "                    f'Episode: {i_episode} | Steps: {t_episode + 1} | Return: {G.item():5.2f} | Epsilon: {eps:4.2f}'\n",
    "                )\n",
    "\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                mask = obs['action_mask']\n",
    "                eps = exploration.value(t_total)\n",
    "                if np.random.rand() < eps:\n",
    "                    action = env.action_spaces[agent].sample(mask)\n",
    "                else:\n",
    "                    action_probs = dqn_models[curr_agent](torch.tensor(obs['observation'].flatten(), dtype=torch.float))\n",
    "                    #set actionprobs at index of invalid actions to -inf\n",
    "                    action_probs[mask == 0] = -float('inf')   \n",
    "                    action = torch.argmax(action_probs).item()\n",
    "                # Use the action to advance the environment by one step\n",
    "                env.step(action)\n",
    "                # Get the next state\n",
    "                next_state, reward, done, _, _ = env.last()\n",
    "                # Store the transition into the replay memory\n",
    "                memories[curr_agent].add(obs['observation'], action, reward, next_state['observation'], done)\n",
    "                # Update the progress bar\n",
    "                t_episode += 1\n",
    "\n",
    "            \n",
    "            # YOUR CODE HERE:  once every 4 steps,\n",
    "            #  * sample a batch from the replay memory\n",
    "            #  * perform a batch update (use the train_dqn_batch() method!)\n",
    "            \n",
    "            if t_total % 2 == 0:\n",
    "                batch = memories[curr_agent].sample(batch_size)\n",
    "                loss = train_dqn_batch(optimizers[curr_agent], batch, dqn_models[curr_agent], dqn_targets[curr_agent], gamma)\n",
    "                losses[curr_agent].append(loss)\n",
    "\n",
    "            # YOUR CODE HERE:  once every 10_000 steps,\n",
    "            #  * update the target network (use the dqn_model.state_dict() and\n",
    "            #    dqn_target.load_state_dict() methods!)\n",
    "            \n",
    "            # Update the target network\n",
    "            if t_total % 1_000 == 0:\n",
    "                dqn_targets[curr_agent].load_state_dict(dqn_models[curr_agent].state_dict())\n",
    "        \n",
    "\n",
    "    for agent in ['agent_0', 'agent_1']:\n",
    "        saved_models[agent]['100_0'] = copy.deepcopy(dqn_models[agent])\n",
    "\n",
    "\n",
    "    return (\n",
    "        saved_models,\n",
    "        returns,\n",
    "        lengths,\n",
    "        losses,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20000, 18])\n",
      "torch.Size([20000, 18])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode: 8622 | Steps: 1 | Return: -1.95 | Epsilon: 0.67:   6%| | 8622/150000 [02:00<32:54, 71.60it/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m exploration \u001b[39m=\u001b[39m ExponentialSchedule(\u001b[39m1.0\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m100_000\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# this should take about 90-120 minutes on a generic 4-core laptop\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m dqn_models, returns, lengths, losses \u001b[39m=\u001b[39m train_dqn_adversarial(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     env,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     num_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     num_saves\u001b[39m=\u001b[39;49mnum_saves,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     replay_size\u001b[39m=\u001b[39;49mreplay_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     replay_prepopulate_steps\u001b[39m=\u001b[39;49mreplay_prepopulate_steps,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     exploration\u001b[39m=\u001b[39;49mexploration,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     gamma\u001b[39m=\u001b[39;49mgamma,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(dqn_models) \u001b[39m==\u001b[39m num_saves\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mall\u001b[39m(\u001b[39misinstance\u001b[39m(value, DQN) \u001b[39mfor\u001b[39;00m value \u001b[39min\u001b[39;00m dqn_models\u001b[39m.\u001b[39mvalues())\n",
      "\u001b[1;32m/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=147'>148</a>\u001b[0m \u001b[39mif\u001b[39;00m t_total \u001b[39m%\u001b[39m \u001b[39m2\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=148'>149</a>\u001b[0m     batch \u001b[39m=\u001b[39m memories[curr_agent]\u001b[39m.\u001b[39msample(batch_size)\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=149'>150</a>\u001b[0m     loss \u001b[39m=\u001b[39m train_dqn_batch(optimizers[curr_agent], batch, dqn_models[curr_agent], dqn_targets[curr_agent], gamma)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=150'>151</a>\u001b[0m     losses[curr_agent]\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=152'>153</a>\u001b[0m \u001b[39m# YOUR CODE HERE:  once every 10_000 steps,\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=153'>154</a>\u001b[0m \u001b[39m#  * update the target network (use the dqn_model.state_dict() and\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=154'>155</a>\u001b[0m \u001b[39m#    dqn_target.load_state_dict() methods!)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=155'>156</a>\u001b[0m \n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=156'>157</a>\u001b[0m \u001b[39m# Update the target network\u001b[39;00m\n",
      "\u001b[1;32m/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb Cell 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"Perform a single batch-update step on the given DQN model.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m:param optimizer: nn.optim.Optimizer instance.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m:rtype: float  The scalar loss associated with this batch.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# YOUR CODE HERE:  compute the values and target_values tensors using the\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# given models and the batch of data.\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m values \u001b[39m=\u001b[39m dqn_model(batch\u001b[39m.\u001b[39;49mstates)\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m, batch\u001b[39m.\u001b[39mactions)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Select the maximum Q-value for each next state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     max_next_values \u001b[39m=\u001b[39m dqn_target(batch\u001b[39m.\u001b[39mnext_states)\u001b[39m.\u001b[39mmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newMlEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb Cell 13\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(states\u001b[39m.\u001b[39mshape) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput states tensor has invalid shape \u001b[39m\u001b[39m{\u001b[39;00mstates\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39massert\u001b[39;00m states\u001b[39m.\u001b[39mshape[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_dim, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mInput states shape \u001b[39m\u001b[39m{\u001b[39;00mstates\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m does not match expected state_dim \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate_dim\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m action_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(states)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/eliasmann/Documents/RL/LOLA_Opponent_Modeling/main.ipynb#X30sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action_values\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newMlEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newMlEnv/lib/python3.10/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newMlEnv/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/newMlEnv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = envs['tictactoe']\n",
    "env.render = False\n",
    "gamma = 0.99\n",
    "\n",
    "# we train for many time-steps;  as usual, you can decrease this during development / debugging.\n",
    "# but make sure to restore it to 1_500_000 before submitting.\n",
    "num_steps = 150_000\n",
    "num_saves = 5  # save models at 0%, 25%, 50%, 75% and 100% of training\n",
    "\n",
    "replay_size = 20_000\n",
    "replay_prepopulate_steps = 5_000\n",
    "\n",
    "batch_size = 32\n",
    "exploration = ExponentialSchedule(1.0, 0.01, 100_000)\n",
    "\n",
    "# this should take about 90-120 minutes on a generic 4-core laptop\n",
    "dqn_models, returns, lengths, losses = train_dqn_adversarial(\n",
    "    env,\n",
    "    num_steps,\n",
    "    num_saves=num_saves,\n",
    "    replay_size=replay_size,\n",
    "    replay_prepopulate_steps=replay_prepopulate_steps,\n",
    "    batch_size=batch_size,\n",
    "    exploration=exploration,\n",
    "    gamma=gamma,\n",
    ")\n",
    "\n",
    "assert len(dqn_models) == num_saves\n",
    "assert all(isinstance(value, DQN) for value in dqn_models.values())\n",
    "\n",
    "# saving computed models to disk, so that we can load and visualize them later.\n",
    "checkpoint = {key: dqn.custom_dump() for key, dqn in dqn_models.items()}\n",
    "torch.save(checkpoint, f'checkpoint_{env.spec.id}.pt')\n",
    "\n",
    "#save the returns, lengths and losses to disk\n",
    "os.makedirs(env.spec.id, exist_ok=False)\n",
    "np.save(f'{env.spec.id}/returns.npy', returns)\n",
    "np.save(f'{env.spec.id}/lengths.npy', lengths)\n",
    "np.save(f'{env.spec.id}/losses.npy', losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Opponent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_OM(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, * num_layers, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        #define the put dimension to take in the state and predicted oppponent action values\n",
    "        input_dim = state_dim + action_dim\n",
    "\n",
    "        #define the layers of the model\n",
    "        layers = []\n",
    "\n",
    "        #input layerNo\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "\n",
    "        #hidden layers\n",
    "        for _ in range(num_layers - 2):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "\n",
    "        #output layer\n",
    "        layers.append(nn.Linear(hidden_dim, action_dim))\n",
    "        layers.append(nn.Softmax(dim=-1))\n",
    "\n",
    "        #create a sequential model\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, state):\n",
    "        action_values = self.model(state)\n",
    "        return action_values\n",
    "\n",
    "    # utility methods for cloning and storing models.  DO NOT EDIT\n",
    "    @classmethod\n",
    "    def custom_load(cls, data):\n",
    "        model = cls(*data['args'], **data['kwargs'])\n",
    "        model.load_state_dict(data['state_dict'])\n",
    "        return model\n",
    "\n",
    "    def custom_dump(self):\n",
    "        return {\n",
    "            'args': (self.state_dim, self.action_dim),\n",
    "            'kwargs': {\n",
    "                'num_layers': self.num_layers,\n",
    "                'hidden_dim': self.hidden_dim,\n",
    "            },\n",
    "            'state_dict': self.state_dict(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the DQN_OM training procedure\n",
    "def train_dqn_om(env, num_players, num_inference_steps, num_training_steps, state_dim, action_dim, hidden_dim):\n",
    "    # Create DQN_OM agents for each player\n",
    "    agents = [DQN_OM(state_dim, action_dim, num_layers=3, hidden_dim=hidden_dim), DQN_OM(state_dim, action_dim, num_layers=3, hidden_dim=hidden_dim)]\n",
    "    num_agents = len(agents)\n",
    "\n",
    "    # Create optimizers for each agent\n",
    "    optimizers = [optim.Adam(agent.parameters(), lr=0.001) for agent in agents]\n",
    "\n",
    "    # Placeholder for opponent action values (z̃other)\n",
    "    opponent_action_values = [torch.zeros((num_players, action_dim)) for _ in range(num_agents)]\n",
    "\n",
    "    for t in range(num_training_steps):\n",
    "        for k in range(num_players):\n",
    "            agent = agents[k]\n",
    "            optimizer = optimizers[k]\n",
    "\n",
    "            # Reset the game environment\n",
    "            state = env.reset()[0]\n",
    "\n",
    "            for step in range(episode_length):\n",
    "                acting_agent = k\n",
    "                non_acting_agent = (k + 1) % num_players  # Assuming a circular opponent order\n",
    "\n",
    "                s_self = game.get_state(acting_agent)\n",
    "                s_other = game.get_state(non_acting_agent)\n",
    "                z_other = opponent_action_values[acting_agent]\n",
    "\n",
    "                z_other = torch.nn.functional.gumbel_softmax(z_other, tau=1.0, hard=False)\n",
    "\n",
    "                # Predict the action values for the acting agent\n",
    "                combined_state = torch.cat((s_self, z_other), dim=-1)\n",
    "                pi_self, _ = agent(combined_state)\n",
    "\n",
    "                # Sample an action for the acting agent\n",
    "                a_self = torch.multinomial(pi_self, 1).squeeze().item()\n",
    "\n",
    "                game.take_action(a_self, acting_agent)\n",
    "\n",
    "                # Perform inference steps for the non-acting agent\n",
    "                for _ in range(num_inference_steps):\n",
    "                    z_other = torch.nn.functional.gumbel_softmax(z_other, tau=1.0, hard=False)\n",
    "                    pi_other, _ = agents[non_acting_agent](s_other, z_other, s_self)\n",
    "\n",
    "                    # Calculate loss and backpropagate\n",
    "                    loss = torch.nn.functional.cross_entropy(pi_other, a_self)\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                opponent_action_values[acting_agent] = z_other\n",
    "\n",
    "            # Update opponent action values for the next round\n",
    "            opponent_action_values[acting_agent] = z_other\n",
    "\n",
    "        # Update policies for all players\n",
    "        for k in range(num_players):\n",
    "            optimizer = optimizers[k]\n",
    "            optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "newMlEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
